# Training configuration
epochs: 2
batch_size: 32
num_workers: 4
learning_rate: 0.001
optimizer: adam  # 'adam', 'sgd', 'adamw'
loss_function: combined  # 'combined', 'dice', 'bce', 'focal', 'jaccard'
precision: 32-true  # '16-mixed', '32-true'
use_gpu: true
gpu_ids: [0]
gradient_clip_val: 0.0
accumulate_grad_batches: 1

# Learning rate scheduler
lr_scheduler:
  enabled: true
  factor: 0.1
  patience: 5
  min_lr: 0.000001
  monitor: val_loss

# Early stopping
early_stopping:
  enabled: true
  patience: 10
  monitor: val_loss
  min_delta: 0.001
  mode: min

# Checkpointing
checkpointing:
  enabled: true
  save_top_k: 3
  monitor: val_loss
  mode: min

# Class weights for imbalanced data
use_class_weights: true